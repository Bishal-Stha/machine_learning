{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vex99np2wFVt"
      },
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jun 14 12:01:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 2050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   44C    P8              3W /   55W |     194MiB /   4096MiB |      5%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A            1656    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A            6340    C+G   ...ms\\Microsoft VS Code\\Code.exe      N/A      |\n",
            "|    0   N/A  N/A            8300    C+G   ...y\\StartMenuExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A            9052    C+G   ...indows\\System32\\ShellHost.exe      N/A      |\n",
            "|    0   N/A  N/A            9424    C+G   ..._cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
            "|    0   N/A  N/A            9788    C+G   ...64__8wekyb3d8bbwe\\Copilot.exe      N/A      |\n",
            "|    0   N/A  N/A            9856    C+G   ...5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
            "|    0   N/A  N/A           11560    C+G   ....0.3296.68\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A           13272    C+G   ...8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A           17648      C   ...s\\Python\\Python310\\python.exe      N/A      |\n",
            "|    0   N/A  N/A           21976    C+G   ...4__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFX7tc1w-en"
      },
      "source": [
        "### 1. What are 3 areas in industry where computer vision is currently being used?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Areas in Industry where computer vision is currently being used are:\n",
            "1. Image Recognition\n",
            "2. autonomous vehicles\n",
            "3. Disease classification\n"
          ]
        }
      ],
      "source": [
        "cv_applications = [\"Image Recognition\", \"autonomous vehicles\", \"Disease classification\"]\n",
        "print(\"Areas in Industry where computer vision is currently being used are:\")\n",
        "for i,cv_application in enumerate(cv_applications):\n",
        "    print(f\"{i+1}. {cv_application}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBK-WI6YxDYa"
      },
      "source": [
        "### 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "outputs": [],
      "source": [
        "print(f\"Overfitting in machine learning means a model doesn't properly gets trained rather it starts to memorize the data instead of learning patterns. Hence during testing on unseen data it performs horrible. That it what Overfitting mean in machine learning.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYFEqw8xK26"
      },
      "source": [
        "### 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ways to prevent overfitting in machine learning\n",
            "1. Hyperparameter-tuning\n",
            "2. Cross-validation\n",
            "3. Feature selection\n",
            "4. trying out different models\n",
            "5. regularization\n",
            "6. increase time of training and testing\n"
          ]
        }
      ],
      "source": [
        "prev_overft = [\"Hyperparameter-tuning\", \"Cross-validation\",\"Feature selection\", \"trying out different models\",\"regularization\",\"increase time of training and testing\"]\n",
        "\n",
        "print(\"Ways to prevent overfitting in machine learning\")\n",
        "for i,prev in enumerate(prev_overft):\n",
        "    print(f\"{i+1}. {prev}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdEEFEqxM-8"
      },
      "source": [
        "### 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ‚úÖ‚úÖ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvf-3pODxXYI"
      },
      "source": [
        "### 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "outputs": [],
      "source": [
        "# Import PyTorch\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load the data (your existing code is correct)\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZW-uAbxe_F"
      },
      "source": [
        "### 6. Visualize at least 5 different samples of the MNIST training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QVFsYi1PbItE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 5 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training samples: 60000\n",
            "Test samples: 10000\n",
            "Image shape: torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(5):\n",
        "    # Get a sample (returns tuple of image tensor and label)\n",
        "    image, label = train_data[i]\n",
        "    \n",
        "    plt.subplot(1, 5, i+1)\n",
        "    plt.imshow(image.squeeze(), cmap='gray')\n",
        "    plt.title(f'Label: {label}')\n",
        "    plt.axis(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Print dataset information\n",
        "print(f\"Training samples: {len(train_data)}\")\n",
        "print(f\"Test samples: {len(test_data)}\")\n",
        "print(f\"Image shape: {train_data[0][0].shape}\")  # First image tensor shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPDzW0wxhi3"
      },
      "source": [
        "### 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x28952723df0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x28952721c60>)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    dataset= train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        "    )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    dataset= test_data,\n",
        "    shuffle=False,\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCVfXk5xjYS"
      },
      "source": [
        "### 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "outputs": [],
      "source": [
        "class MNISTModelV0(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_shape: int,\n",
        "                 hidden_unit: int,\n",
        "                 output_shape: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_unit, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        # We'll determine the in_features dynamically üñêÔ∏è GPT Wrote it.\n",
        "        self._dummy_input = torch.randn(1, input_shape, 28, 28)\n",
        "        self._out_shape = self._get_flattened_shape(self._dummy_input, hidden_unit)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self._out_shape, output_shape)\n",
        "        )\n",
        "\n",
        "    def _get_flattened_shape(self, x, hidden_units): #üñêÔ∏è GPT wrote it.\n",
        "        with torch.no_grad():\n",
        "            x = self.conv_block_1(x)\n",
        "            x = self.conv_block_2(x)\n",
        "            return x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_3zUr7xlhy"
      },
      "source": [
        "### 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1CsHhPpxp1w"
      },
      "source": [
        "### 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQwzqlBWxrpG"
      },
      "source": [
        "### 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6bDhoWxt2y"
      },
      "source": [
        "### 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHS20cNTxwSi"
      },
      "source": [
        "### 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
